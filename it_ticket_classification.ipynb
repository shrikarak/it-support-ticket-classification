{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for IT Support: Automated Ticket Classification\n",
    "\n",
    "**Copyright (c) 2026 Shrikara Kaudambady**\n",
    "\n",
    "This notebook implements a machine learning model to automatically classify IT support tickets based on their descriptions. By automating this process, IT departments can improve efficiency, reduce manual effort, and ensure tickets are routed to the correct teams faster.\n",
    "\n",
    "**Problem:** Manually sorting through hundreds of IT tickets is time-consuming and prone to error.\n",
    "**Solution:** A Natural Language Processing (NLP) model that reads a ticket's description and assigns it to a predefined category (e.g., `Hardware`, `Software`, `Network`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports\n",
    "\n",
    "First, we import the necessary libraries for data manipulation, text processing, model training, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Settings\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "We will load our synthetic dataset of IT support tickets and perform a brief analysis to understand its structure and the distribution of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('it_support_tickets.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Category Distribution:\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='category', data=df, palette='viridis')\n",
    "plt.title('Distribution of Ticket Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "This is a critical step in any NLP task. We will clean the ticket descriptions by:\n",
    "1. Converting text to lowercase.\n",
    "2. Removing punctuation and numbers.\n",
    "3. Removing common English 'stopwords' (e.g., 'the', 'a', 'is').\n",
    "4. Applying stemming to reduce words to their root form (e.g., 'running' -> 'run')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords from NLTK\n",
    "# This only needs to be done once\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function\n",
    "df['cleaned_description'] = df['description'].apply(clean_text)\n",
    "print(\"Original vs. Cleaned Text:\")\n",
    "print(\"Original:\", df['description'][0])\n",
    "print(\"Cleaned:\", df['cleaned_description'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering (TF-IDF Vectorization)\n",
    "\n",
    "Machine learning models can't understand raw text. We need to convert the cleaned text into numerical vectors. We'll use the **TF-IDF (Term Frequency-Inverse Document Frequency)** method. TF-IDF gives higher weight to words that are frequent in a specific document but rare across all documents, making them good indicators of the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "X = vectorizer.fit_transform(df['cleaned_description'])\n",
    "y = df['category']\n",
    "\n",
    "print(\"Shape of TF-IDF matrix:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We will split our data into training and testing sets and train a `LogisticRegression` model. This model is a strong and interpretable baseline for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "We'll now evaluate our model's performance on the unseen test data using accuracy, a classification report, and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.ylabel('Actual Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing with New Tickets\n",
    "\n",
    "Let's see how our model performs on a few new, unseen ticket descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tickets = [\n",
    "    \"My email client is not syncing new emails from the server.\",\n",
    "    \"The power button on my desktop computer is stuck.\",\n",
    "    \"I can't access the internet from my laptop, the wifi icon has a yellow triangle.\"\n",
    "]\n",
    "\n",
    "cleaned_new_tickets = [clean_text(ticket) for ticket in new_tickets]\n",
    "new_tickets_tfidf = vectorizer.transform(cleaned_new_tickets)\n",
    "\n",
    "predictions = model.predict(new_tickets_tfidf)\n",
    "\n",
    "for ticket, category in zip(new_tickets, predictions):\n",
    "    print(f'Ticket: \"{ticket}\" -> Predicted Category: {category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "We have successfully built a simple but effective machine learning model to classify IT support tickets. With a high accuracy on our test set, this model can serve as a solid foundation for an automated ticket routing system.\n",
    "\n",
    "### Next Steps\n",
    "- **Use more data:** A real-world system would require thousands of tickets to be robust.\n",
    "- **Advanced Models:** Explore more complex models like Support Vector Machines (SVM), or deep learning models (LSTMs, Transformers like BERT) for potentially higher accuracy.\n",
    "- **Deployment:** Wrap the model in a REST API (using a framework like Flask or FastAPI) so that it can be integrated with a real IT ticketing system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
